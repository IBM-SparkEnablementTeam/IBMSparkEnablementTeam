{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "sc = SparkContext(appName=\"CTRPredictor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size is 100000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'0,1,1,5,0,1382,4,15,2,181,1,2,,2,68fd1e64,80e26c9b,fb936136,7b4723c4,25c83c98,7e0ccccf,de7995b8,1f89b562,a73ee510,a8cd5504,b2cb9c98,37c9c164,2824a5f6,1adce6ef,8ba8b39a,891b62e7,e5ba7672,f54016b9,21ddcdc9,b1252a9d,07b5194c,,3a171ecb,c5c50484,e8b83407,9727dd16',\n",
       " u'0,2,0,44,1,102,8,2,2,4,1,1,,4,68fd1e64,f0cf0024,6f67f7e5,41274cd7,25c83c98,fe6b92e5,922afcc0,0b153874,a73ee510,2b53e5fb,4f1b46f3,623049e6,d7020589,b28479f6,e6c5b5cd,c92f3b61,07c540c4,b04e4670,21ddcdc9,5840adea,60f6221e,,3a171ecb,43f13e8b,e8b83407,731c3655']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 'file:/root/notebooks/CTRPrediction/dac_sample.txt'\n",
    "sample_data = sc.textFile(sample, 2).map(lambda x: x.replace('\\t', ',')) \n",
    "\n",
    "\n",
    "print \"Data size is {}\".format(sample_data.count())\n",
    "sample_data.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79911 10075 10014 100000\n"
     ]
    }
   ],
   "source": [
    "# Use randomSplit with weights and seed to get training, test and validation data sets\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "\n",
    "rawTrainData, rawValidationData, rawTestData = sample_data.randomSplit(weights, seed)\n",
    "# Cache the data\n",
    "rawTrainData.cache()\n",
    "rawValidationData.cache()\n",
    "rawTestData.cache()\n",
    "\n",
    "nTrain = (rawTrainData.count())\n",
    "nVal = rawValidationData.count()\n",
    "nTest = rawTestData.count()\n",
    "print nTrain, nVal, nTest, nTrain + nVal + nTest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CatFeature1: string (nullable = true)\n",
      " |-- CatFeature10: string (nullable = true)\n",
      " |-- CatFeature11: string (nullable = true)\n",
      " |-- CatFeature12: string (nullable = true)\n",
      " |-- CatFeature13: string (nullable = true)\n",
      " |-- CatFeature2: string (nullable = true)\n",
      " |-- CatFeature3: string (nullable = true)\n",
      " |-- CatFeature4: string (nullable = true)\n",
      " |-- CatFeature5: string (nullable = true)\n",
      " |-- CatFeature6: string (nullable = true)\n",
      " |-- CatFeature7: string (nullable = true)\n",
      " |-- CatFeature8: string (nullable = true)\n",
      " |-- CatFeature9: string (nullable = true)\n",
      " |-- IntFeature1: string (nullable = true)\n",
      " |-- IntFeature10: string (nullable = true)\n",
      " |-- IntFeature11: string (nullable = true)\n",
      " |-- IntFeature12: string (nullable = true)\n",
      " |-- IntFeature13: string (nullable = true)\n",
      " |-- IntFeature2: string (nullable = true)\n",
      " |-- IntFeature3: string (nullable = true)\n",
      " |-- IntFeature4: string (nullable = true)\n",
      " |-- IntFeature5: string (nullable = true)\n",
      " |-- IntFeature6: string (nullable = true)\n",
      " |-- IntFeature7: string (nullable = true)\n",
      " |-- IntFeature8: string (nullable = true)\n",
      " |-- IntFeature9: string (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import SQLContext \n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "import math\n",
    "\n",
    "# Parse data and create data frames\n",
    "def parseData(data, sqlContext):\n",
    " \n",
    "    #Split the csv file by comma and convert each line to a tuple.\n",
    "    parts = data.map(lambda l: l.split(\",\", -1))\n",
    "    features = parts.map(lambda p: Row(Label=(p[0]), IntFeature1=(p[1]), IntFeature2=(p[2]), IntFeature3=p[3],\n",
    "                                IntFeature4=(p[4]), IntFeature5=(p[5]), IntFeature6=p[6], IntFeature7=p[7], \n",
    "                                IntFeature8=(p[8]), IntFeature9=(p[9]), IntFeature10=p[10], IntFeature11=p[11],\n",
    "                                IntFeature12=(p[12]), IntFeature13=(p[13]), CatFeature1=p[14], CatFeature2=p[15],\n",
    "                                CatFeature3=p[16], CatFeature4=p[17],CatFeature5=p[18], CatFeature6=p[19],\n",
    "                                CatFeature7=p[20], CatFeature8=p[21],CatFeature9=p[22], CatFeature10=p[23],\n",
    "                                CatFeature11=p[24], CatFeature12=p[25],CatFeature13=p[26]))\n",
    "                                 \n",
    "\n",
    "    # Apply the schema to the RDD.\n",
    "    return sqlContext.createDataFrame(features)\n",
    "\n",
    "# sc is an existing SparkContext.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Register the DataFrame as a table.\n",
    "schemaClicks = parseData(sample_data, sqlContext)\n",
    "schemaClicks.registerTempTable(\"clicks\")\n",
    "schemaClicks.printSchema()\n",
    "\n",
    "schemaClicksTrain = parseData(rawTrainData, sqlContext)\n",
    "schemaClicksTrain.registerTempTable(\"clicksTrain\")\n",
    "\n",
    "\n",
    "schemaClicksTest = parseData(rawTestData, sqlContext)\n",
    "schemaClicksTest.registerTempTable(\"clicksTest\")\n",
    "\n",
    "\n",
    "schemaClicksValidation = parseData(rawValidationData, sqlContext)\n",
    "schemaClicksValidation.registerTempTable(\"clicksValidation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CatFeature1: string (nullable = true)\n",
      " |-- CatFeature10: string (nullable = true)\n",
      " |-- CatFeature11: string (nullable = true)\n",
      " |-- CatFeature12: string (nullable = true)\n",
      " |-- CatFeature13: string (nullable = true)\n",
      " |-- CatFeature2: string (nullable = true)\n",
      " |-- CatFeature3: string (nullable = true)\n",
      " |-- CatFeature4: string (nullable = true)\n",
      " |-- CatFeature5: string (nullable = true)\n",
      " |-- CatFeature6: string (nullable = true)\n",
      " |-- CatFeature7: string (nullable = true)\n",
      " |-- CatFeature8: string (nullable = true)\n",
      " |-- CatFeature9: string (nullable = true)\n",
      " |-- IntFeature1: double (nullable = true)\n",
      " |-- IntFeature2: double (nullable = true)\n",
      " |-- IntFeature3: double (nullable = true)\n",
      " |-- IntFeature4: double (nullable = true)\n",
      " |-- IntFeature5: double (nullable = true)\n",
      " |-- IntFeature6: double (nullable = true)\n",
      " |-- IntFeature7: double (nullable = true)\n",
      " |-- IntFeature8: double (nullable = true)\n",
      " |-- IntFeature9: double (nullable = true)\n",
      " |-- IntFeature10: double (nullable = true)\n",
      " |-- IntFeature11: double (nullable = true)\n",
      " |-- IntFeature12: double (nullable = true)\n",
      " |-- IntFeature13: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cast numeric features to double \n",
    "df=schemaClicks .withColumn(\"IntFeature1tmp\",schemaClicks.IntFeature1.cast('double')).drop(\"IntFeature1\") .withColumnRenamed(\"IntFeature1tmp\",\"IntFeature1\").withColumn(\"IntFeature2tmp\",schemaClicks.IntFeature2.cast('double')).drop(\"IntFeature2\") .withColumnRenamed(\"IntFeature2tmp\",\"IntFeature2\").withColumn(\"IntFeature3tmp\",schemaClicks.IntFeature3.cast('double')).drop(\"IntFeature3\") .withColumnRenamed(\"IntFeature3tmp\",\"IntFeature3\").withColumn(\"IntFeature4tmp\",schemaClicks.IntFeature4.cast('double')).drop(\"IntFeature4\") .withColumnRenamed(\"IntFeature4tmp\",\"IntFeature4\").withColumn(\"IntFeature5tmp\",schemaClicks.IntFeature5.cast('double')).drop(\"IntFeature5\") .withColumnRenamed(\"IntFeature5tmp\",\"IntFeature5\").withColumn(\"IntFeature6tmp\",schemaClicks.IntFeature6.cast('double')).drop(\"IntFeature6\") .withColumnRenamed(\"IntFeature6tmp\",\"IntFeature6\").withColumn(\"IntFeature7tmp\",schemaClicks.IntFeature7.cast('double')).drop(\"IntFeature7\") .withColumnRenamed(\"IntFeature7tmp\",\"IntFeature7\").withColumn(\"IntFeature8tmp\",schemaClicks.IntFeature8.cast('double')).drop(\"IntFeature8\") .withColumnRenamed(\"IntFeature8tmp\",\"IntFeature8\").withColumn(\"IntFeature9tmp\",schemaClicks.IntFeature9.cast('double')).drop(\"IntFeature9\") .withColumnRenamed(\"IntFeature9tmp\",\"IntFeature9\").withColumn(\"IntFeature10tmp\",schemaClicks.IntFeature10.cast('double')).drop(\"IntFeature10\") .withColumnRenamed(\"IntFeature10tmp\",\"IntFeature10\").withColumn(\"IntFeature11tmp\",schemaClicks.IntFeature11.cast('double')).drop(\"IntFeature11\") .withColumnRenamed(\"IntFeature11tmp\",\"IntFeature11\").withColumn(\"IntFeature12tmp\",schemaClicks.IntFeature12.cast('double')).drop(\"IntFeature12\") .withColumnRenamed(\"IntFeature12tmp\",\"IntFeature12\").withColumn(\"IntFeature13tmp\",schemaClicks.IntFeature13.cast('double')).drop(\"IntFeature13\") .withColumnRenamed(\"IntFeature13tmp\",\"IntFeature13\").withColumn(\"label\",schemaClicks.Label.cast('double')).drop(\"Label\")\n",
    "\n",
    "dfTrain=schemaClicksTrain .withColumn(\"IntFeature1tmp\",schemaClicksTrain.IntFeature1.cast('double')).drop(\"IntFeature1\") .withColumnRenamed(\"IntFeature1tmp\",\"IntFeature1\").withColumn(\"IntFeature2tmp\",schemaClicksTrain.IntFeature2.cast('double')).drop(\"IntFeature2\") .withColumnRenamed(\"IntFeature2tmp\",\"IntFeature2\").withColumn(\"IntFeature3tmp\",schemaClicksTrain.IntFeature3.cast('double')).drop(\"IntFeature3\") .withColumnRenamed(\"IntFeature3tmp\",\"IntFeature3\").withColumn(\"IntFeature4tmp\",schemaClicksTrain.IntFeature4.cast('double')).drop(\"IntFeature4\") .withColumnRenamed(\"IntFeature4tmp\",\"IntFeature4\").withColumn(\"IntFeature5tmp\",schemaClicksTrain.IntFeature5.cast('double')).drop(\"IntFeature5\") .withColumnRenamed(\"IntFeature5tmp\",\"IntFeature5\").withColumn(\"IntFeature6tmp\",schemaClicksTrain.IntFeature6.cast('double')).drop(\"IntFeature6\") .withColumnRenamed(\"IntFeature6tmp\",\"IntFeature6\").withColumn(\"IntFeature7tmp\",schemaClicksTrain.IntFeature7.cast('double')).drop(\"IntFeature7\") .withColumnRenamed(\"IntFeature7tmp\",\"IntFeature7\").withColumn(\"IntFeature8tmp\",schemaClicksTrain.IntFeature8.cast('double')).drop(\"IntFeature8\") .withColumnRenamed(\"IntFeature8tmp\",\"IntFeature8\").withColumn(\"IntFeature9tmp\",schemaClicksTrain.IntFeature9.cast('double')).drop(\"IntFeature9\") .withColumnRenamed(\"IntFeature9tmp\",\"IntFeature9\").withColumn(\"IntFeature10tmp\",schemaClicksTrain.IntFeature10.cast('double')).drop(\"IntFeature10\") .withColumnRenamed(\"IntFeature10tmp\",\"IntFeature10\").withColumn(\"IntFeature11tmp\",schemaClicksTrain.IntFeature11.cast('double')).drop(\"IntFeature11\") .withColumnRenamed(\"IntFeature11tmp\",\"IntFeature11\").withColumn(\"IntFeature12tmp\",schemaClicksTrain.IntFeature12.cast('double')).drop(\"IntFeature12\") .withColumnRenamed(\"IntFeature12tmp\",\"IntFeature12\").withColumn(\"IntFeature13tmp\",schemaClicksTrain.IntFeature13.cast('double')).drop(\"IntFeature13\") .withColumnRenamed(\"IntFeature13tmp\",\"IntFeature13\").withColumn(\"label\",schemaClicksTrain.Label.cast('double')).drop(\"Label\")\n",
    "dfTest=schemaClicksTest .withColumn(\"IntFeature1tmp\",schemaClicksTest.IntFeature1.cast('double')).drop(\"IntFeature1\") .withColumnRenamed(\"IntFeature1tmp\",\"IntFeature1\").withColumn(\"IntFeature2tmp\",schemaClicksTest.IntFeature2.cast('double')).drop(\"IntFeature2\") .withColumnRenamed(\"IntFeature2tmp\",\"IntFeature2\").withColumn(\"IntFeature3tmp\",schemaClicksTest.IntFeature3.cast('double')).drop(\"IntFeature3\") .withColumnRenamed(\"IntFeature3tmp\",\"IntFeature3\").withColumn(\"IntFeature4tmp\",schemaClicksTest.IntFeature4.cast('double')).drop(\"IntFeature4\") .withColumnRenamed(\"IntFeature4tmp\",\"IntFeature4\").withColumn(\"IntFeature5tmp\",schemaClicksTest.IntFeature5.cast('double')).drop(\"IntFeature5\") .withColumnRenamed(\"IntFeature5tmp\",\"IntFeature5\").withColumn(\"IntFeature6tmp\",schemaClicksTest.IntFeature6.cast('double')).drop(\"IntFeature6\") .withColumnRenamed(\"IntFeature6tmp\",\"IntFeature6\").withColumn(\"IntFeature7tmp\",schemaClicksTest.IntFeature7.cast('double')).drop(\"IntFeature7\") .withColumnRenamed(\"IntFeature7tmp\",\"IntFeature7\").withColumn(\"IntFeature8tmp\",schemaClicksTest.IntFeature8.cast('double')).drop(\"IntFeature8\") .withColumnRenamed(\"IntFeature8tmp\",\"IntFeature8\").withColumn(\"IntFeature9tmp\",schemaClicksTest.IntFeature9.cast('double')).drop(\"IntFeature9\") .withColumnRenamed(\"IntFeature9tmp\",\"IntFeature9\").withColumn(\"IntFeature10tmp\",schemaClicksTest.IntFeature10.cast('double')).drop(\"IntFeature10\") .withColumnRenamed(\"IntFeature10tmp\",\"IntFeature10\").withColumn(\"IntFeature11tmp\",schemaClicksTest.IntFeature11.cast('double')).drop(\"IntFeature11\") .withColumnRenamed(\"IntFeature11tmp\",\"IntFeature11\").withColumn(\"IntFeature12tmp\",schemaClicksTest.IntFeature12.cast('double')).drop(\"IntFeature12\") .withColumnRenamed(\"IntFeature12tmp\",\"IntFeature12\").withColumn(\"IntFeature13tmp\",schemaClicksTest.IntFeature13.cast('double')).drop(\"IntFeature13\") .withColumnRenamed(\"IntFeature13tmp\",\"IntFeature13\").withColumn(\"label\",schemaClicksTest.Label.cast('double')).drop(\"Label\")\n",
    "dfValidation=schemaClicksValidation .withColumn(\"IntFeature1tmp\",schemaClicksValidation.IntFeature1.cast('double')).drop(\"IntFeature1\") .withColumnRenamed(\"IntFeature1tmp\",\"IntFeature1\").withColumn(\"IntFeature2tmp\",schemaClicksValidation.IntFeature2.cast('double')).drop(\"IntFeature2\") .withColumnRenamed(\"IntFeature2tmp\",\"IntFeature2\").withColumn(\"IntFeature3tmp\",schemaClicksValidation.IntFeature3.cast('double')).drop(\"IntFeature3\") .withColumnRenamed(\"IntFeature3tmp\",\"IntFeature3\").withColumn(\"IntFeature4tmp\",schemaClicksValidation.IntFeature4.cast('double')).drop(\"IntFeature4\") .withColumnRenamed(\"IntFeature4tmp\",\"IntFeature4\").withColumn(\"IntFeature5tmp\",schemaClicksValidation.IntFeature5.cast('double')).drop(\"IntFeature5\") .withColumnRenamed(\"IntFeature5tmp\",\"IntFeature5\").withColumn(\"IntFeature6tmp\",schemaClicksValidation.IntFeature6.cast('double')).drop(\"IntFeature6\") .withColumnRenamed(\"IntFeature6tmp\",\"IntFeature6\").withColumn(\"IntFeature7tmp\",schemaClicksValidation.IntFeature7.cast('double')).drop(\"IntFeature7\") .withColumnRenamed(\"IntFeature7tmp\",\"IntFeature7\").withColumn(\"IntFeature8tmp\",schemaClicksValidation.IntFeature8.cast('double')).drop(\"IntFeature8\") .withColumnRenamed(\"IntFeature8tmp\",\"IntFeature8\").withColumn(\"IntFeature9tmp\",schemaClicksValidation.IntFeature9.cast('double')).drop(\"IntFeature9\") .withColumnRenamed(\"IntFeature9tmp\",\"IntFeature9\").withColumn(\"IntFeature10tmp\",schemaClicksValidation.IntFeature10.cast('double')).drop(\"IntFeature10\") .withColumnRenamed(\"IntFeature10tmp\",\"IntFeature10\").withColumn(\"IntFeature11tmp\",schemaClicksValidation.IntFeature11.cast('double')).drop(\"IntFeature11\") .withColumnRenamed(\"IntFeature11tmp\",\"IntFeature11\").withColumn(\"IntFeature12tmp\",schemaClicksValidation.IntFeature12.cast('double')).drop(\"IntFeature12\") .withColumnRenamed(\"IntFeature12tmp\",\"IntFeature12\").withColumn(\"IntFeature13tmp\",schemaClicksValidation.IntFeature13.cast('double')).drop(\"IntFeature13\") .withColumnRenamed(\"IntFeature13tmp\",\"IntFeature13\").withColumn(\"label\",schemaClicksValidation.Label.cast('double')).drop(\"Label\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "cat1Indexer = StringIndexer(inputCol=\"CatFeature1\", outputCol=\"indexedCat1\")\n",
    "cat1Encoder = OneHotEncoder(inputCol=\"indexedCat1\", outputCol=\"CatVector1\")\n",
    "\n",
    "cat2Indexer = StringIndexer(inputCol=\"CatFeature2\", outputCol=\"indexedCat2\")\n",
    "cat2Encoder = OneHotEncoder(inputCol=\"indexedCat2\", outputCol=\"CatVector2\")\n",
    "\n",
    "cat3Indexer = StringIndexer(inputCol=\"CatFeature3\", outputCol=\"indexedCat3\")\n",
    "cat3Encoder = OneHotEncoder(inputCol=\"indexedCat3\", outputCol=\"CatVector3\")\n",
    "\n",
    "cat4Indexer = StringIndexer(inputCol=\"CatFeature4\", outputCol=\"indexedCat4\")\n",
    "cat4Encoder = OneHotEncoder(inputCol=\"indexedCat4\", outputCol=\"CatVector4\")\n",
    "\n",
    "cat5Indexer = StringIndexer(inputCol=\"CatFeature5\", outputCol=\"indexedCat5\")\n",
    "cat5Encoder = OneHotEncoder(inputCol=\"indexedCat5\", outputCol=\"CatVector5\")\n",
    "\n",
    "cat6Indexer = StringIndexer(inputCol=\"CatFeature6\", outputCol=\"indexedCat6\")\n",
    "cat6Encoder = OneHotEncoder(inputCol=\"indexedCat6\", outputCol=\"CatVector6\")\n",
    "\n",
    "cat7Indexer = StringIndexer(inputCol=\"CatFeature7\", outputCol=\"indexedCat7\")\n",
    "cat7Encoder = OneHotEncoder(inputCol=\"indexedCat7\", outputCol=\"CatVector7\")\n",
    "\n",
    "cat8Indexer = StringIndexer(inputCol=\"CatFeature8\", outputCol=\"indexedCat8\")\n",
    "cat8Encoder = OneHotEncoder(inputCol=\"indexedCat8\", outputCol=\"CatVector8\")\n",
    "\n",
    "cat9Indexer = StringIndexer(inputCol=\"CatFeature9\", outputCol=\"indexedCat9\")\n",
    "cat9Encoder = OneHotEncoder(inputCol=\"indexedCat9\", outputCol=\"CatVector9\")\n",
    "\n",
    "cat10Indexer = StringIndexer(inputCol=\"CatFeature10\", outputCol=\"indexedCat10\")\n",
    "cat10Encoder = OneHotEncoder(inputCol=\"indexedCat10\", outputCol=\"CatVector10\")\n",
    "\n",
    "cat11Indexer = StringIndexer(inputCol=\"CatFeature11\", outputCol=\"indexedCat11\")\n",
    "cat11Encoder = OneHotEncoder(inputCol=\"indexedCat11\", outputCol=\"CatVector11\")\n",
    "\n",
    "cat12Indexer = StringIndexer(inputCol=\"CatFeature12\", outputCol=\"indexedCat12\")\n",
    "cat12Encoder = OneHotEncoder(inputCol=\"indexedCat12\", outputCol=\"CatVector12\")\n",
    "\n",
    "cat13Indexer = StringIndexer(inputCol=\"CatFeature13\", outputCol=\"indexedCat13\")\n",
    "cat13Encoder = OneHotEncoder(inputCol=\"indexedCat13\", outputCol=\"CatVector13\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "featureAssembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "            \"IntFeature1\", \"IntFeature2\", \"IntFeature3\", \"IntFeature4\", \"IntFeature5\", \"IntFeature6\", \"IntFeature7\", \"IntFeature8\",\n",
    "            \"IntFeature9\", \"IntFeature10\", \"IntFeature11\", \"IntFeature12\", \"IntFeature13\", \n",
    "            \"CatVector1\", \"CatVector2\",\"CatVector3\",\"CatVector4\",\"CatVector5\",\"CatVector6\",\n",
    "            \"CatVector7\",\"CatVector8\",\"CatVector9\",\"CatVector10\",\"CatVector11\",\"CatVector12\",\"CatVector13\"],\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|        features|\n",
      "+----------------+\n",
      "| (470,[2],[1.0])|\n",
      "| (470,[0],[1.0])|\n",
      "| (470,[2],[1.0])|\n",
      "| (470,[0],[1.0])|\n",
      "| (470,[1],[1.0])|\n",
      "| (470,[1],[1.0])|\n",
      "| (470,[5],[1.0])|\n",
      "| (470,[0],[1.0])|\n",
      "| (470,[4],[1.0])|\n",
      "| (470,[0],[1.0])|\n",
      "| (470,[2],[1.0])|\n",
      "| (470,[0],[1.0])|\n",
      "|(470,[72],[1.0])|\n",
      "| (470,[0],[1.0])|\n",
      "| (470,[0],[1.0])|\n",
      "| (470,[1],[1.0])|\n",
      "| (470,[1],[1.0])|\n",
      "| (470,[0],[1.0])|\n",
      "| (470,[6],[1.0])|\n",
      "| (470,[0],[1.0])|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Playing with Feature Selection\n",
    "from pyspark.ml import Pipeline\n",
    "fAssembler = VectorAssembler(\n",
    "    inputCols=[\"CatVector1\"],\n",
    "    outputCol=\"features\")\n",
    "pipelineTmp = Pipeline(stages=[cat1Indexer, cat1Encoder,fAssembler])\n",
    "\n",
    "modelTmp = pipelineTmp.fit(dfTrain)\n",
    "tmp = modelTmp.transform(dfTest).select(\"features\")\n",
    "\n",
    "tmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "    \n",
    "pipeline = Pipeline(stages=[cat1Indexer, cat2Indexer, cat3Indexer, cat4Indexer, cat5Indexer,cat6Indexer, cat7Indexer,\n",
    "                            cat8Indexer, cat9Indexer, cat10Indexer, cat11Indexer, cat12Indexer, cat13Indexer,\n",
    "                            cat1Encoder, cat2Encoder, cat3Encoder, cat4Encoder, cat5Encoder, cat6Encoder, cat7Encoder,\n",
    "                            cat8Encoder, cat9Encoder, cat10Encoder, cat11Encoder, cat12Encoder, cat13Encoder,\n",
    "                            fAssembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_4d6ba71b48e1665f203e\n"
     ]
    }
   ],
   "source": [
    "# Train model.  This also runs the indexers, encoders and assembler\n",
    "model = pipeline.fit(dfTrain)\n",
    "\n",
    "treeModel = model.stages[27]\n",
    "print treeModel # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+----------+--------------------+--------------------+\n",
      "|        features|label|prediction|       rawPrediction|         probability|\n",
      "+----------------+-----+----------+--------------------+--------------------+\n",
      "| (470,[2],[1.0])|  0.0|       0.0|[1.21013445009279...|[0.77032273757685...|\n",
      "| (470,[0],[1.0])|  0.0|       0.0|[1.22247147156137...|[0.77249819004175...|\n",
      "| (470,[2],[1.0])|  0.0|       0.0|[1.21013445009279...|[0.77032273757685...|\n",
      "| (470,[0],[1.0])|  0.0|       0.0|[1.22247147156137...|[0.77249819004175...|\n",
      "| (470,[1],[1.0])|  0.0|       0.0|[1.21806470675759...|[0.77172279445516...|\n",
      "| (470,[1],[1.0])|  0.0|       0.0|[1.21806470675759...|[0.77172279445516...|\n",
      "| (470,[5],[1.0])|  0.0|       0.0|[1.28438632487356...|[0.78319550087281...|\n",
      "| (470,[0],[1.0])|  1.0|       0.0|[1.22247147156137...|[0.77249819004175...|\n",
      "| (470,[4],[1.0])|  0.0|       0.0|[1.17124174410751...|[0.76336939346512...|\n",
      "| (470,[0],[1.0])|  0.0|       0.0|[1.22247147156137...|[0.77249819004175...|\n",
      "| (470,[2],[1.0])|  0.0|       0.0|[1.21013445009279...|[0.77032273757685...|\n",
      "| (470,[0],[1.0])|  1.0|       0.0|[1.22247147156137...|[0.77249819004175...|\n",
      "|(470,[72],[1.0])|  0.0|       0.0|[0.17692017333184...|[0.54411503385543...|\n",
      "| (470,[0],[1.0])|  1.0|       0.0|[1.22247147156137...|[0.77249819004175...|\n",
      "| (470,[0],[1.0])|  0.0|       0.0|[1.22247147156137...|[0.77249819004175...|\n",
      "| (470,[1],[1.0])|  0.0|       0.0|[1.21806470675759...|[0.77172279445516...|\n",
      "| (470,[1],[1.0])|  0.0|       0.0|[1.21806470675759...|[0.77172279445516...|\n",
      "| (470,[0],[1.0])|  0.0|       0.0|[1.22247147156137...|[0.77249819004175...|\n",
      "| (470,[6],[1.0])|  1.0|       0.0|[1.25336512491324...|[0.77788183678703...|\n",
      "| (470,[0],[1.0])|  0.0|       0.0|[1.22247147156137...|[0.77249819004175...|\n",
      "+----------------+-----+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(dfTest).select(\"features\", \"label\", \"prediction\", \"rawPrediction\", \"probability\")\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3144.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 117.0 failed 1 times, most recent failure: Lost task 0.0 in stage 117.0 (TID 213, localhost): org.apache.spark.SparkException: Unseen label: 6e4a368f.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:131)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:126)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:71)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.CreateStruct$$anonfun$eval$2.apply(complexTypes.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.CreateStruct$$anonfun$eval$2.apply(complexTypes.scala:75)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat org.apache.spark.sql.catalyst.expressions.CreateStruct.eval(complexTypes.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.CreateStruct.eval(complexTypes.scala:56)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:68)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:52)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:201)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:56)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-78c9908acb24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     numFolds=numFolds)\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mcvModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrossval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfTrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcvModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfTest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/iop/4.1.0.0/spark/python/pyspark/ml/pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m/usr/iop/4.1.0.0/spark/python/pyspark/ml/tuning.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m                 \u001b[1;31m# TODO: duplicate evaluator to take extra params from input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m                 \u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meva\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0mbestIndex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/iop/4.1.0.0/spark/python/pyspark/ml/evaluation.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/iop/4.1.0.0/spark/python/pyspark/ml/evaluation.pyc\u001b[0m in \u001b[0;36m_evaluate\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \"\"\"\n\u001b[0;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/iop/4.1.0.0/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/iop/4.1.0.0/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o3144.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 117.0 failed 1 times, most recent failure: Lost task 0.0 in stage 117.0 (TID 213, localhost): org.apache.spark.SparkException: Unseen label: 6e4a368f.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:131)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:126)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:71)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.CreateStruct$$anonfun$eval$2.apply(complexTypes.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.CreateStruct$$anonfun$eval$2.apply(complexTypes.scala:75)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat org.apache.spark.sql.catalyst.expressions.CreateStruct.eval(complexTypes.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.CreateStruct.eval(complexTypes.scala:56)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:68)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:52)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:201)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:56)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    }
   ],
   "source": [
    "# Try Cross Validator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from  pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()\n",
    "numFolds=2\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=numFolds)\n",
    "\n",
    "cvModel = crossval.fit(dfTrain)\n",
    "evaluator.evaluate(cvModel.transform(dfTest))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 119.0 failed 1 times, most recent failure: Lost task 1.0 in stage 119.0 (TID 216, localhost): org.apache.spark.SparkException: Unseen label: c77d22cf.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:131)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:126)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:71)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.CreateStruct$$anonfun$eval$2.apply(complexTypes.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.CreateStruct$$anonfun$eval$2.apply(complexTypes.scala:75)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat org.apache.spark.sql.catalyst.expressions.CreateStruct.eval(complexTypes.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.CreateStruct.eval(complexTypes.scala:56)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:68)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:52)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:120)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:111)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:111)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.to(SerDeUtil.scala:111)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toBuffer(SerDeUtil.scala:111)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toArray(SerDeUtil.scala:111)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:885)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:885)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-4f4184623d84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlabelsAndScores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"probability\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlabelsAndWeights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabelsAndScores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlabelsAndWeights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlabelsByWeight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabelsAndWeights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/iop/4.1.0.0/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \"\"\"\n\u001b[0;32m    280\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjavaToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/iop/4.1.0.0/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/iop/4.1.0.0/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 119.0 failed 1 times, most recent failure: Lost task 1.0 in stage 119.0 (TID 216, localhost): org.apache.spark.SparkException: Unseen label: c77d22cf.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:131)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$4.apply(StringIndexer.scala:126)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:71)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.CreateStruct$$anonfun$eval$2.apply(complexTypes.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.CreateStruct$$anonfun$eval$2.apply(complexTypes.scala:75)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat org.apache.spark.sql.catalyst.expressions.CreateStruct.eval(complexTypes.scala:75)\n\tat org.apache.spark.sql.catalyst.expressions.CreateStruct.eval(complexTypes.scala:56)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf$$anonfun$2.apply(ScalaUdf.scala:70)\n\tat org.apache.spark.sql.catalyst.expressions.ScalaUdf.eval(ScalaUdf.scala:960)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:68)\n\tat org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:52)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:120)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:111)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:111)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.to(SerDeUtil.scala:111)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toBuffer(SerDeUtil.scala:111)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toArray(SerDeUtil.scala:111)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:885)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:885)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1767)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    }
   ],
   "source": [
    "# Visualize ROC plot\n",
    "\n",
    "labelsAndScores = prediction.select(\"label\", \"probability\")\n",
    "labelsAndWeights = labelsAndScores.collect()\n",
    "labelsAndWeights.sort(key=lambda (k, v): v, reverse=True)\n",
    "labelsByWeight = np.array([k for (k, v) in labelsAndWeights])\n",
    "\n",
    "length = labelsByWeight.size\n",
    "truePositives = labelsByWeight.cumsum()\n",
    "numPositive = truePositives[-1]\n",
    "falsePositives = np.arange(1.0, length + 1, 1.) - truePositives\n",
    "\n",
    "truePositiveRate = truePositives / numPositive\n",
    "falsePositiveRate = falsePositives / (length - numPositive)\n",
    "\n",
    "# Generate layout and plot data\n",
    "fig, ax = preparePlot(np.arange(0., 1.1, 0.1), np.arange(0., 1.1, 0.1))\n",
    "ax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\n",
    "ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "ax.set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.plot(falsePositiveRate, truePositiveRate, color='#8cbfd0', linestyle='-', linewidth=3.)\n",
    "plt.plot((0., 1.), (0., 1.), linestyle='--', color='#d6ebf2', linewidth=2.)  # Baseline model\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def computeLogLoss(p, y):\n",
    "    \"\"\"Calculates the value of log loss for a given probabilty and label.\n",
    "    Note:\n",
    "        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n",
    "        and when p is 1 we need to subtract a small value (epsilon) from it.\n",
    "    Args:\n",
    "        p (float): A probabilty between 0 and 1.\n",
    "        y (int): A label.  Takes on the values 0 and 1.\n",
    "    Returns:\n",
    "        float: The log loss value.\n",
    "    \"\"\"\n",
    "    epsilon = 10e-12\n",
    "    if y == 1:\n",
    "        return -log(epsilon + p) if p == 0 else -log(p)\n",
    "    elif y == 0:\n",
    "        return -log(1 - p + epsilon) if p == 1 else -log(1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ROC for BinaryClassification\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "#>>> scoreAndLabels = sc.parallelize([ import \n",
    "#...     (0.1, 0.0), (0.1, 1.0), (0.4, 0.0), (0.6, 0.0), (0.6, 1.0), (0.6, 1.0), (0.8, 1.0)], 2)\n",
    "#>>> metrics = BinaryClassificationMetrics(scoreAndLabels)\n",
    "#>>> metrics.areaUnderROC\n",
    "#0.70...\n",
    "#>>> metrics.areaUnderPR\n",
    "#0.83...\n",
    "#>>> metrics.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To optimize piepline creation using concise code\n",
    "import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler, \n",
    "\n",
    "OneHotEncoder}\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val data = sqlContext.read.parquet(\"s3n://map2-test/forecaster/intermediate_data\")\n",
    "\n",
    "val df = data.select(\"win\",\"bid_price\",\"domain\",\"size\", \"form_factor\").na.drop()\n",
    "\n",
    "\n",
    "//indexing columns\n",
    "val stringColumns = Array(\"domain\",\"size\", \"form_factor\")\n",
    "val index_transformers: Array[org.apache.spark.ml.PipelineStage] = stringColumns.map(\n",
    "  cname => new StringIndexer()\n",
    "    .setInputCol(cname)\n",
    "    .setOutputCol(s\"${cname}_index\")\n",
    ")\n",
    "\n",
    "// Add the rest of your pipeline like VectorAssembler and algorithm\n",
    "val index_pipeline = new Pipeline().setStages(index_transformers)\n",
    "val index_model = index_pipeline.fit(df)\n",
    "val df_indexed = index_model.transform(df)\n",
    "\n",
    "\n",
    "//encoding columns\n",
    "val indexColumns  = df_indexed.columns.filter(x => x contains \"index\")\n",
    "val one_hot_encoders: Array[org.apache.spark.ml.PipelineStage] = indexColumns.map(\n",
    "    cname => new OneHotEncoder()\n",
    "     .setInputCol(cname)\n",
    "     .setOutputCol(s\"${cname}_vec\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "val one_hot_pipeline = new Pipeline().setStages(one_hot_encoders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
